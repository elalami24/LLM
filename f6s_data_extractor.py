#!/usr/bin/env python3
"""
Extracteur de donn√©es F6S √† partir du HTML r√©cup√©r√©
Analyse et structure les donn√©es des programmes
"""

import json
import re
import logging
from pathlib import Path
from datetime import datetime
from bs4 import BeautifulSoup
import pandas as pd

# Configuration
OUTPUT_DIR = Path("extracted_data")
LOG_FILE = "extraction.log"

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class F6SDataExtractor:
    def __init__(self, html_content=None, html_file=None):
        """Initialise l'extracteur avec du HTML"""
        self.html_content = html_content
        self.html_file = html_file
        self.soup = None
        self.programs = []
        
        # Cr√©er le dossier de sortie
        OUTPUT_DIR.mkdir(exist_ok=True)
        
        # Charger le HTML
        if html_file and Path(html_file).exists():
            with open(html_file, 'r', encoding='utf-8') as f:
                self.html_content = f.read()
        
        if self.html_content:
            self.soup = BeautifulSoup(self.html_content, 'html.parser')
            logger.info(f"HTML charg√©, taille: {len(self.html_content)} caract√®res")
        else:
            logger.error("Aucun contenu HTML fourni")

    def extract_programs(self):
        """Extrait tous les programmes de la page"""
        if not self.soup:
            logger.error("Aucun contenu HTML √† analyser")
            return []
        
        logger.info("D√©but de l'extraction des programmes...")
        
        # Chercher les √©l√©ments de programmes
        program_containers = self.soup.find_all('div', class_='bordered-list-item result-item')
        
        logger.info(f"Trouv√© {len(program_containers)} conteneurs de programmes")
        
        for i, container in enumerate(program_containers):
            try:
                program = self._extract_single_program(container, i)
                if program:
                    self.programs.append(program)
                    logger.debug(f"Programme {i+1} extrait: {program.get('title', 'Sans titre')}")
            except Exception as e:
                logger.error(f"Erreur lors de l'extraction du programme {i+1}: {e}")
        
        logger.info(f"Extraction termin√©e: {len(self.programs)} programmes extraits")
        return self.programs

    def _extract_single_program(self, container, index):
        """Extrait les donn√©es d'un seul programme"""
        program = {
            'id': index + 1,
            'title': '',
            'description': '',
            'location': '',
            'deadline': '',
            'date_range': '',
            'url': '',
            'apply_url': '',
            'info_url': '',
            'funding_amount': '',
            'equity': '',
            'organization_image': '',
            'verified': False,
            'type': '',
            'markets': [],
            'raw_text': container.get_text(strip=True) if container else ''
        }
        
        try:
            # Extraire le titre
            title_elem = container.find('div', class_='title')
            if title_elem:
                title_link = title_elem.find('a')
                if title_link:
                    program['title'] = title_link.get_text(strip=True)
                    program['info_url'] = title_link.get('href', '')
                    
                    # V√©rifier si v√©rifi√©
                    if title_elem.find('span', class_='verified-badge'):
                        program['verified'] = True
            
            # Extraire les d√©tails (localisation, dates)
            subtitle_elem = container.find('div', class_='subtitle')
            if subtitle_elem:
                subtitle_text = subtitle_elem.get_text(strip=True)
                
                # Extraire la date et la localisation
                if '‚Ä¢' in subtitle_text:
                    parts = subtitle_text.split('‚Ä¢')
                    if len(parts) >= 2:
                        program['date_range'] = parts[0].strip()
                        program['location'] = parts[1].strip()
                elif subtitle_text:
                    # Si pas de s√©parateur, c'est probablement juste la localisation
                    program['location'] = subtitle_text
            
            # Extraire la description/d√©tails
            details_elem = container.find('div', class_='details')
            if details_elem:
                details_text = details_elem.get_text(strip=True)
                if details_text:
                    program['description'] = details_text
                    
                    # Extraire les march√©s des d√©tails
                    if 'Funds Startups in' in details_text:
                        markets_text = details_text.replace('Funds Startups in', '').strip()
                        program['markets'] = [market.strip() for market in markets_text.split(',')]
            
            # Extraire les informations de financement
            result_extra = container.find('div', class_='result-extra')
            if result_extra:
                # Montant de financement
                funding_spans = result_extra.find_all('span', class_='emphasis')
                for span in funding_spans:
                    text = span.get_text(strip=True)
                    if '$' in text:
                        program['funding_amount'] = text
                    elif '%' in text:
                        program['equity'] = text
            
            # Extraire les URLs d'action
            result_action = container.find('div', class_='result-action')
            if result_action:
                action_links = result_action.find_all('a')
                for link in action_links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True).lower()
                    
                    if 'apply' in text:
                        program['apply_url'] = href
                        program['type'] = 'application'
                    elif 'book' in text:
                        program['apply_url'] = href
                        program['type'] = 'booking'
                    elif 'more info' in text:
                        program['info_url'] = href
            
            # Extraire le deadline depuis le data-overlay
            deadline_elem = container.find('span', class_='data-overlay')
            if deadline_elem:
                deadline_text = deadline_elem.get_text(strip=True)
                if deadline_text.startswith('by'):
                    program['deadline'] = deadline_text
            
            # Extraire l'image de l'organisation
            img_elem = container.find('img', class_='profile')
            if img_elem:
                program['organization_image'] = img_elem.get('src', '')
            
            # Nettoyer les URLs (ajouter le domaine si relatif)
            for url_field in ['url', 'apply_url', 'info_url']:
                if program[url_field] and program[url_field].startswith('/'):
                    program[url_field] = 'https://www.f6s.com' + program[url_field]
            
            return program
            
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction d√©taill√©e du programme {index+1}: {e}")
            return program

    def extract_page_metadata(self):
        """Extrait les m√©tadonn√©es de la page"""
        metadata = {
            'extraction_date': datetime.now().isoformat(),
            'page_title': '',
            'page_description': '',
            'total_results': 0,
            'url': '',
            'extraction_method': 'html_parsing'
        }
        
        if not self.soup:
            return metadata
        
        try:
            # Titre de la page
            title_elem = self.soup.find('title')
            if title_elem:
                metadata['page_title'] = title_elem.get_text(strip=True)
            
            # Description
            desc_elem = self.soup.find('meta', attrs={'name': 'description'})
            if desc_elem:
                metadata['page_description'] = desc_elem.get('content', '')
            
            # URL canonique
            canonical_elem = self.soup.find('link', attrs={'rel': 'canonical'})
            if canonical_elem:
                metadata['url'] = canonical_elem.get('href', '')
            
            # Nombre total de r√©sultats (depuis le script)
            script_tags = self.soup.find_all('script')
            for script in script_tags:
                if script.string and 'results' in script.string:
                    # Chercher le pattern "XXXX results"
                    results_match = re.search(r'(\d+)\s+results', script.string)
                    if results_match:
                        metadata['total_results'] = int(results_match.group(1))
                        break
            
        except Exception as e:
            logger.error(f"Erreur lors de l'extraction des m√©tadonn√©es: {e}")
        
        return metadata

    def save_to_json(self, filename=None):
        """Sauvegarde les donn√©es au format JSON"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = OUTPUT_DIR / f"f6s_programs_{timestamp}.json"
        
        data = {
            'metadata': self.extract_page_metadata(),
            'programs': self.programs,
            'summary': {
                'total_extracted': len(self.programs),
                'with_funding': len([p for p in self.programs if p.get('funding_amount')]),
                'verified': len([p for p in self.programs if p.get('verified')]),
                'with_deadline': len([p for p in self.programs if p.get('deadline')]),
                'locations': list(set([p.get('location') for p in self.programs if p.get('location')])),
                'markets': list(set([market for p in self.programs for market in p.get('markets', [])]))
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Donn√©es sauvegard√©es dans {filename}")
        return filename

    def save_to_csv(self, filename=None):
        """Sauvegarde les donn√©es au format CSV"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = OUTPUT_DIR / f"f6s_programs_{timestamp}.csv"
        
        if not self.programs:
            logger.warning("Aucun programme √† sauvegarder")
            return None
        
        # Pr√©parer les donn√©es pour CSV
        csv_data = []
        for program in self.programs:
            row = program.copy()
            # Convertir les listes en cha√Ænes
            row['markets'] = '; '.join(program.get('markets', []))
            csv_data.append(row)
        
        # Cr√©er le DataFrame et sauvegarder
        df = pd.DataFrame(csv_data)
        df.to_csv(filename, index=False, encoding='utf-8')
        
        logger.info(f"Donn√©es CSV sauvegard√©es dans {filename}")
        return filename

    def generate_report(self):
        """G√©n√®re un rapport d'extraction"""
        if not self.programs:
            return "Aucun programme extrait"
        
        report = []
        report.append("="*60)
        report.append("RAPPORT D'EXTRACTION F6S")
        report.append("="*60)
        report.append(f"Date d'extraction: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Nombre total de programmes: {len(self.programs)}")
        report.append("")
        
        # Statistiques
        with_funding = [p for p in self.programs if p.get('funding_amount')]
        verified = [p for p in self.programs if p.get('verified')]
        with_deadline = [p for p in self.programs if p.get('deadline')]
        
        report.append("STATISTIQUES:")
        report.append(f"- Programmes avec financement: {len(with_funding)}")
        report.append(f"- Programmes v√©rifi√©s: {len(verified)}")
        report.append(f"- Programmes avec deadline: {len(with_deadline)}")
        report.append("")
        
        # √âchantillon des programmes
        report.append("√âCHANTILLON DES PROGRAMMES:")
        report.append("-" * 40)
        
        for i, program in enumerate(self.programs[:10]):  # Premiers 10
            report.append(f"\n{i+1}. {program.get('title', 'Sans titre')}")
            if program.get('location'):
                report.append(f"   üìç {program['location']}")
            if program.get('funding_amount'):
                report.append(f"   üí∞ {program['funding_amount']}")
            if program.get('deadline'):
                report.append(f"   ‚è∞ {program['deadline']}")
            if program.get('verified'):
                report.append(f"   ‚úÖ V√©rifi√©")
        
        if len(self.programs) > 10:
            report.append(f"\n... et {len(self.programs) - 10} autres programmes")
        
        return "\n".join(report)

def main():
    """Fonction principale pour tester l'extraction"""
    print("EXTRACTEUR DE DONN√âES F6S")
    print("="*50)
    
    # Exemple d'utilisation
    html_file = input("Entrez le chemin du fichier HTML (ou appuyez sur Entr√©e pour utiliser l'exemple): ").strip()
    
    if not html_file:
        # Utiliser le HTML que vous avez fourni
        html_content = """<!-- Ici vous pouvez coller votre HTML -->"""
        extractor = F6SDataExtractor(html_content=html_content)
    else:
        if not Path(html_file).exists():
            print(f"‚ùå Fichier non trouv√©: {html_file}")
            return
        extractor = F6SDataExtractor(html_file=html_file)
    
    # Extraire les programmes
    programs = extractor.extract_programs()
    
    if not programs:
        print("‚ùå Aucun programme extrait")
        return
    
    # Sauvegarder les donn√©es
    json_file = extractor.save_to_json()
    csv_file = extractor.save_to_csv()
    
    # G√©n√©rer et afficher le rapport
    report = extractor.generate_report()
    print("\n" + report)
    
    # Sauvegarder le rapport
    report_file = OUTPUT_DIR / f"extraction_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nüìÅ Fichiers g√©n√©r√©s:")
    print(f"   - JSON: {json_file}")
    print(f"   - CSV: {csv_file}")
    print(f"   - Rapport: {report_file}")

if __name__ == "__main__":
    main()